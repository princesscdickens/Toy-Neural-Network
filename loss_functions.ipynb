{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The standard loss function for neural network - cross-entropy\n",
    "\n",
    "Dec 12 2019\n",
    "\n",
    "This time, your job is to do a few more things with the same network:\n",
    "\n",
    "• You should calculate not just the output, but also the cross-entropy loss and the mean squared loss\n",
    "\n",
    "• NOTE: This time, the initial values should be different from what we did in class, x1 = 0.5 and x2 = 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Softmax Layer\n",
    "\n",
    "\n",
    "Recall the behavior of the softmax layer: given a vector x the ith element in the result of the softmax becomes:\n",
    "\n",
    "def softmax(x):\n",
    "return np.exp(x) / np.sum(np.exp(x))\n",
    "That is, the exponential of the ith element, divided by the sum of the exponentials of all ele- ments. This guarantees that the sum of all elements in the resulting softmax will be 1.\n",
    "\n",
    "2) No bias\n",
    "\n",
    "Note again that we ignore the bias feature and weight for the sake of simplicity in this exercise.\n",
    "\n",
    "3) Nonlinearity\n",
    "\n",
    "Recall that the outputs of the first hidden layer need to be passed through the tanh-function, while the outputs of the second layer do not. This is reflected in the network diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output1:  [0.2069665  0.29131261 0.37136023]\n"
     ]
    }
   ],
   "source": [
    "from math import log, exp, tanh\n",
    "import numpy as np\n",
    "\n",
    "initial = np.array([0.5, 0.4])\n",
    "W1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "\n",
    "# Hidden layer + activation function (tanh)\n",
    "output1 = np.tanh(initial.dot(W1))\n",
    "print('Output1: ', output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output2:  [0.29377055 0.38073448]\n"
     ]
    }
   ],
   "source": [
    "W2 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "\n",
    "# Second hidden layer, NO activation function\n",
    "\n",
    "output2 = output1.dot(W2)\n",
    "print('Output2: ', output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:  [0.47827271 0.52172729]\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "result = softmax(output2)\n",
    "print('Softmax: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy\n",
    "\n",
    "Remember the formula for cross-entropy: given two vectors y and ŷ (where ŷ is the network’s output and y the true, desired value):\n",
    "\n",
    "In this example, the “desired” output vector should be [0,1] (y), that is, the uppermost output (say class 0) should be 0 and the lowermost one (say class 1) should be 1. Your job is then to calculate the output of the entire neural network, which should produce a vector of two values[?,?] (ŷ), after which you should calculate the cross-entropy.\n",
    "\n",
    "You can solve this completely manually, but it may be helpful to write small helper functions in Python to make sure you get your calculations right. Be sure to turn in all your work, not just final numbers. To use the exp, log, and tanh functions in Python, import the following: from math import log, exp, tanh\n",
    "\n",
    "Of course, you can use the numpy equivalents as well.\n",
    "\n",
    "Also, note that the log-function should be base 2. That means that you need to specify the base as the second argument if you call log. For example, to calculate log(4) to base two, you would do:\n",
    "\n",
    "log(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy:  0.9386321906685277\n"
     ]
    }
   ],
   "source": [
    "gold = np.array([0,1])\n",
    "\n",
    "def calculateLoss(y, x):\n",
    "    sum1 = 0\n",
    "    for i in range(len(x)):\n",
    "        sum1 += y[i] * log(1/x[i], 2) \n",
    "    return sum1\n",
    "\n",
    "loss1 = calculateLoss(gold, result)\n",
    "print(\"Cross-entropy: \", loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE)\n",
    "The sum of the squares of errors\n",
    "\n",
    "* gold = 1, prediction = 0.8\n",
    "* MSE = 1/100 * (0.8 - 1)^2 = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.22874478312422142\n"
     ]
    }
   ],
   "source": [
    "# gold = np.array([0,1])\n",
    "# result = [0.47827271 0.52172729]\n",
    "\n",
    "def calcMSE(gold, result):\n",
    "    for i in range(len(gold)):\n",
    "        total = 0\n",
    "        number = result[i] - gold[i]\n",
    "        total += number * number\n",
    "    return total\n",
    "    \n",
    "MSE = calcMSE(gold, result)\n",
    "print('MSE: ', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
